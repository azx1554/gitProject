{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640a0e71",
   "metadata": {},
   "source": [
    "# RAG法律助手\n",
    "**本节课学习目标:**\n",
    "1. 什么是RAG？\n",
    "2. 为什么要文档切片？\n",
    "3. 表示学习与嵌入\n",
    "4. 向量化过程分析\n",
    "5. LangChain集成Ollama\n",
    "6. RAG检索底层实现细节\n",
    "7. 企业级RAG项目方案分析\n",
    "\n",
    "## 1,什么是RAG？\n",
    "   **RAG(Retrieval Augmented Generation)顾名思义，通过检索的方法来增强生成模型的能力**\n",
    "   ![](./img/rag原理.jpg)\n",
    "\n",
    "### 为什么需要RAG？\n",
    "   **大模言模型(LLM)因为预训练的过程,加之本身是基于预测去生成内容的系统,导致它必然会有如下缺陷**  \n",
    "     **- 知识时效性**  \n",
    "     **- 推理局限性**  \n",
    "     **- 专业领域盲区**  \n",
    "     **- 幻觉问题**  \n",
    "\n",
    "\n",
    "## 2,RAG项目实战\n",
    "### 搭建过程\n",
    "  #### 一，语料库准备\n",
    "  \n",
    "  #### 二，加载文档并且按需求切片\n",
    "    首先要对文档进行数据清洗，去除不必要的数据(噪声数据)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入项目环境\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap,RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from dotenv import load_dotenv\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "# 1. 加载和分割文档\n",
    "loader = TextLoader(\"./documents/中华人民共和国刑法.txt\")\n",
    "# \n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "text_list = []\n",
    "for i in splits:\n",
    "    print(i)\n",
    "    text_list.append(i)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=\"\"\"请根据检索的上下文回答问题。\n",
    "    \n",
    "    上下文：\n",
    "    {context}\n",
    "\n",
    "    问题：\n",
    "    {query}\n",
    "\n",
    "    答案：\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe33219",
   "metadata": {},
   "source": [
    "#### 三，加载本地Embedding模型\n",
    "  ##### 加载本地模型后，将文档切片传入模型进行向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b514e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 创建向量存储(使用本地嵌入模型)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"D:\\\\AIProjects\\\\modelscope\\\\BAAI\\\\bge-base-zh-v1___5\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "                                    #文档切片\n",
    "                                    splits, \n",
    "                                    #本地Embedding模型\n",
    "                                    embeddings,\n",
    "                                    #通过Chroma将向量存储到本地\n",
    "                                    persist_directory=\"./chrome_db\"\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630890b",
   "metadata": {},
   "source": [
    "#### 四，通过Ollama集成本地LLM大模型并提问  \n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3387ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 构建检索链-通过Ollama集成本地千问大语言模型\n",
    "llm = Ollama(\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    #温度参数\n",
    "    temperature = 0.0,\n",
    "    top_k = 5\n",
    "    )\n",
    "\n",
    "\n",
    "# 4. 构成LangChain问答链\n",
    "qa_chain = RunnableMap(\n",
    "    {\"context\": vectorstore.as_retriever(), \"query\": RunnablePassthrough()},\n",
    ") | prompt | llm | StrOutputParser()\n",
    "\n",
    "# 5. 提出问题,LLM回答\n",
    "result = qa_chain.invoke(\"张三抢劫银行获得十万元,将受到什么判罚？\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1610fef",
   "metadata": {},
   "source": [
    "#### 五，引入ragas评估回答指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def evaluate_rag(query, true_answer=None):\n",
    "    \"\"\"\n",
    "    完整RAG流程+评估\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    contexts = [doc.page_content for doc in retriever.invoke(query)]\n",
    "\n",
    "    qa_chain = RunnableMap(\n",
    "        {\"context\": retriever, \"query\": RunnablePassthrough()},\n",
    "    ) | prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # 2. 生成阶段\n",
    "    answer = qa_chain.invoke(query)\n",
    "    \n",
    "    true_answer = ''' \n",
    "        根据中华人民共和国刑法的相关规定，某某如果抢劫获得十万元，并且符合其他法定情节（如多次抢夺、数额巨大等），那么他可能会面临以下刑罚:\n",
    "        处三年以上十年以下有期徒刑，并处罚金；有严重情节的，处十年以上有期徒刑、无期徒刑或者死刑，并处罚金或者没收财产.   \n",
    "    '''\n",
    "    # 3. 准备评估数据\n",
    "    eval_data = {\n",
    "        \"question\": [query],\n",
    "        \"answer\": [answer],\n",
    "        \"contexts\": [contexts],\n",
    "        \"ground_truth\":[true_answer]\n",
    "    }\n",
    "    \n",
    "    # 4. 运行评估\n",
    "    dataset = Dataset.from_dict(eval_data)\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall if true_answer else None,\n",
    "        context_precision\n",
    "    ]\n",
    "    \n",
    "    result = evaluate(dataset, metrics=metrics,llm=llm,embeddings=embeddings)\n",
    "\n",
    "    result.upload()\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"evaluation\": result\n",
    "    }\n",
    "\n",
    "evaluate_rag(\"张三抢劫银行获得十万元,将受到什么判罚？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f8182",
   "metadata": {},
   "source": [
    "#### 六，优化检索过程提升回答准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d19742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 综合优化示例：重排序 + 查询扩展\n",
    "from langchain.retrievers import MultiQueryRetriever,ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "'''\n",
    "    1. 查询扩展\n",
    "        MultiQueryRetriever是一种自动化提示调整过程的工具,它通过使用大型语言模型(LLM)从不同的角度生成多个查询视角,\n",
    "    并为每个查询检索相关文档,最终通过所有查询的结果的独特并集,得到一个更大、更有潜力的相关文档集合。\n",
    "'''\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10}),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "''' \n",
    "    2. 检索后压缩/重排序\n",
    "        LLMChainExtractor是一个从大语言模型中提取信息的工具,它可以将LLM的输出进行压缩,去除冗余信息,保留关键内容\n",
    "        from_llm(llm)是LLMChainExtractor的一个类方法,用于创建一个LLMChainExtractor实例,\n",
    "    并将传入的语言模型与该实例关联起来。这样,后续就可以使用这个实例对文本进行压缩处理\n",
    "        ContextualCompressionRetriever是一种上下文压缩检索器,它结合了基础检索器和文档压缩器,\n",
    "    能够通过上下文压缩技术，将检索结果优化为用户真正需要的内容，避免浪费资源\n",
    "        压缩的目的是去除检索结果中的冗余信息，减少信息量，同时保留关键内容，以便后续处理。\n",
    "            例子1:\n",
    "            ​原句​​:“我们应该重视教育公平,因为教育能够改变一个人的命运,而公平的教育机会是实现社会公正的重要途径。”\n",
    "            ​​压缩后​​:“教育公平是实现社会公正的重要途径。”\n",
    "            例子2:\n",
    "            原句​​:“地球的自转产生了昼夜交替的现象,地球围绕太阳的公转产生了四季变化。”\n",
    "​            ​压缩后​​:“地球自转产生昼夜交替,公转产生四季变化。”\n",
    "        重排序是对检索到的文档进行重新评估和排序，以确保最相关且最有价值的检索结果能够优先被用作回答查询的上下文输入。\n",
    "\n",
    "    压缩与重排序的结合\n",
    "        ​基础检索​​:先使用基础的向量存储检索器进行初步查询,从文档库中找到与查询相关的文档。\n",
    "    ​    压缩检索​​:使用LLMChainExtractor对检索到的文档进行压缩,去除与查询不相关的内容,减少信息量同时保留关键信息。\n",
    "    ​    ​重排序​​:对压缩后的文档进行重新排序,使用重排序模型(如Reranker)对文档的相关性进行更精确的评估,筛选出最相关的文档,\n",
    "    并按照相关性分数进行排序,确保最相关的文档优先被传递给LLM进行处理。\n",
    "'''\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 4. 构成LangChain问答链\n",
    "qa_chain = RunnableMap(\n",
    "    {\"context\": vectorstore.as_retriever(), \"query\": RunnablePassthrough()},\n",
    ") | prompt | llm | StrOutputParser()\n",
    "\n",
    "# 5. 提出问题,LLM回答\n",
    "result = qa_chain.invoke(\"张三抢劫获得十万元,将受到什么判罚？\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
